{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "SEED = 1\n",
    "BATCH_SIZE = 128\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = .2\n",
    "ZDIMS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs\n",
    "\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kmer_path = \"data\\kmers-gzip\\\\output.txt.gz\"\n",
    "\n",
    "kmer_all = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count\n",
      "count  3.388926e+07\n",
      "mean   5.418934e+01\n",
      "std    9.312541e+01\n",
      "min    2.000000e+00\n",
      "25%    3.000000e+00\n",
      "50%    1.000000e+01\n",
      "75%    5.200000e+01\n",
      "max    8.817000e+03\n",
      "             count\n",
      "count  8763.000000\n",
      "mean    923.065617\n",
      "std     320.572167\n",
      "min     700.000000\n",
      "25%     780.000000\n",
      "50%     851.000000\n",
      "75%     968.000000\n",
      "max    8817.000000\n"
     ]
    }
   ],
   "source": [
    "kmer_culled = kmer_all.loc[kmer_all['count'] >= 700]\n",
    "print(kmer_all.describe())\n",
    "\n",
    "print(kmer_culled.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               count\n",
      "count  388884.000000\n",
      "mean        2.296068\n",
      "std         0.511630\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         3.000000\n",
      "max         6.000000\n",
      "             count\n",
      "count  1686.000000\n",
      "mean      4.771056\n",
      "std       1.080067\n",
      "min       2.000000\n",
      "25%       4.000000\n",
      "50%       4.000000\n",
      "75%       6.000000\n",
      "max       6.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kmer_path = \"data\\kmers-gzip\\\\upec-182.txt.gz\"\n",
    "\n",
    "kmer_culled.sort_values(by=['kmer'])\n",
    "\n",
    "kmer_contig1 = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "print(kmer_contig1.describe())\n",
    "kmer_contig1.sort_values(by=\"kmer\",)\n",
    "\n",
    "all = kmer_culled.kmer\n",
    "\n",
    "contig1_parsed = kmer_contig1[kmer_contig1.kmer.isin(all)]\n",
    "\n",
    "print(contig1_parsed.describe())\n",
    "kmer_temp = kmer_culled.copy()\n",
    "kmer_temp['count'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kmer</th>\n",
       "      <th>count_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAACATGATCACCGGTGCTGCTCAGATGG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAACATGATCACCGGTGCTGCTCAGATGGA</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAATGGTTGTTACCCTGATCCACCCGATCG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAACATGATCACCGGTGCTGCTCAGATGGAC</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAATGCGACATGGTTGATGACGAAGAGCTGC</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>GGTAAGGAGGTGATCCAACCGCAGGTTCCCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>GGTCGGCGGTTCGATCCCGTCATCACCCACC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8760</th>\n",
       "      <td>GGTTTAGAACGTCGTGAGACAGTTCGGTCCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8761</th>\n",
       "      <td>GGTGATTAGCTCAGCTGGGAGAGCACCTCCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8762</th>\n",
       "      <td>GGGTGATGACGGGATCGAACCGCCGACCCCC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8763 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 kmer count_x\n",
       "0     AAAAACATGATCACCGGTGCTGCTCAGATGG       4\n",
       "1     AAAACATGATCACCGGTGCTGCTCAGATGGA       4\n",
       "2     AAAATGGTTGTTACCCTGATCCACCCGATCG       4\n",
       "3     AAACATGATCACCGGTGCTGCTCAGATGGAC       4\n",
       "4     AAATGCGACATGGTTGATGACGAAGAGCTGC       4\n",
       "...                               ...     ...\n",
       "8758  GGTAAGGAGGTGATCCAACCGCAGGTTCCCC       0\n",
       "8759  GGTCGGCGGTTCGATCCCGTCATCACCCACC       0\n",
       "8760  GGTTTAGAACGTCGTGAGACAGTTCGGTCCC       0\n",
       "8761  GGTGATTAGCTCAGCTGGGAGAGCACCTCCC       0\n",
       "8762  GGGTGATGACGGGATCGAACCGCCGACCCCC       0\n",
       "\n",
       "[8763 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contig1_merged = contig1_parsed.merge(kmer_temp, how='right',on='kmer')\n",
    "\n",
    "contig1_merged[['kmer', 'count_x']].fillna(value='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CULL_SIZE = 700\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dirname):\n",
    "        files = os.listdir(dirname)\n",
    "        \n",
    "        X, y = [],[]\n",
    "        \n",
    "        for line in files:\n",
    "            if(line == 'output.txt.gz'):\n",
    "                kmer_all = pd.read_csv('data\\kmers-gzip\\\\output.txt.gz' ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "            else:\n",
    "                kmer_path = dirname + '\\\\' + line\n",
    "                kmer_df = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "                X.append(kmer_df)\n",
    "        self.X = X\n",
    "        kmer_culled = kmer_all.loc[kmer_all['count'] >= CULL_SIZE]\n",
    "        kmer_temp = kmer_culled.copy()\n",
    "        kmer_temp['count'] = '0'\n",
    "        self.template = kmer_temp\n",
    "    \n",
    "    def preprocess(self, contig):\n",
    "        contig_parsed = contig[contig.kmer.isin(self.template.kmer)]\n",
    "        \n",
    "        contig_merged = contig_parsed.merge(self.template, how='right',on='kmer')\n",
    "\n",
    "        contig_merged[['kmer', 'count_x']].fillna(value='0')\n",
    "        contig_merged.columns = ['kmer', 'count']\n",
    "        return contig_merged\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.preprocess(self, self.X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize dataset\n",
    "dataset = KmerDataset('data\\kmers-gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate into train/val\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#initialize data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(400, ZDIMS)\n",
    "        self.fc22 = nn.Linear(400, ZDIMS)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(ZDIMS, 400)\n",
    "        \n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        \n",
    "        if self.training:\n",
    "            \n",
    "            std = logvar.mul(.5).exp_()\n",
    "            \n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            \n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "        \n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "    \n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    KLD /= BATCH_SIZE * 784\n",
    "    \n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len (train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.data.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data.item()\n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            n = min(data.size(0), 8)\n",
    "            \n",
    "            comparison = torch.cat([data[:n],\n",
    "                                  recon_batch.view(BATCH_SIZE, 1, 28,28)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                      'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.005470\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.002900\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.002334\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.002219\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.002125\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.002175\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.002150\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.001974\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.001940\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.001812\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.001866\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.001707\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.001701\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.001632\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.001635\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.001681\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.001557\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.001596\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.001592\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.001543\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.001488\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.001444\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.001510\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.001488\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.001523\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.001482\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.001481\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.001443\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.001475\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.001419\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.001427\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.001487\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.001389\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.001317\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.001308\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.001353\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.001341\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.001316\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.001293\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.001326\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.001326\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.001334\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.001345\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.001301\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.001274\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001240\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.001292\n",
      "====> Epoch: 1 Average loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\Anaconda3\\envs\\mGWAS\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0012\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001257\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.001303\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.001268\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.001236\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.001238\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001253\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.001273\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.001244\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.001246\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.001245\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001240\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.001203\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.001214\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.001230\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.001196\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001150\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.001205\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.001189\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.001170\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001238\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.001195\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.001213\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.001221\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.001194\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.001191\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001218\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.001267\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001224\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.001203\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001195\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001197\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.001166\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.001183\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.001118\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.001170\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001195\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.001155\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001147\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.001137\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001145\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.001230\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.001179\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.001176\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.001154\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.001160\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.001169\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001185\n",
      "====> Epoch: 2 Average loss: 0.0012\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001154\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.001153\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.001155\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.001132\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.001152\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001120\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.001145\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.001104\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001169\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001174\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.001166\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001170\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.001183\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001157\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001127\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.001119\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.001113\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.001109\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001144\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001131\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.001133\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.001187\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.001159\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001141\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.001076\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.001155\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.001144\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.001124\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001152\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.001165\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.001122\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.001102\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.001152\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001150\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.001089\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001118\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.001153\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001141\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.001129\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.001151\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.001135\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.001134\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.001083\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.001072\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001117\n",
      "====> Epoch: 3 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001078\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.001132\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.001072\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001130\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.001115\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.001156\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001096\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.001124\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.001112\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.001174\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001121\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001154\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.001129\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.001156\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.001146\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001103\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.001092\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.001082\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.001094\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001125\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001082\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.001117\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.001107\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.001111\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001143\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.001112\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001109\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001083\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.001101\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.001068\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001059\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.001078\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001103\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001105\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.001064\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.001118\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.001096\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.001117\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001074\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.001068\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.001124\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001071\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.001088\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001086\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.001116\n",
      "====> Epoch: 4 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001084\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.001077\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.001095\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001163\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001110\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.001113\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001091\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.001071\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001096\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.001046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aa20a420340d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-a677c3c9b26f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mGWAS\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mGWAS\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "    \n",
    "    sample = Variable(torch.randn(64, ZDIMS))\n",
    "    \n",
    "    if CUDA:\n",
    "        sample = sample.cuda()\n",
    "    \n",
    "    sample = model.decode(sample).cpu()\n",
    "    \n",
    "    \n",
    "    save_image(sample.data.view(64, 1, 28, 28),\n",
    "              'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
