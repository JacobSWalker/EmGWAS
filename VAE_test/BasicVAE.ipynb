{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "SEED = 1\n",
    "BATCH_SIZE = 1\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = .2\n",
    "ZDIMS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "#    batch_size=BATCH_SIZE, shuffle=True, **kwargs\n",
    "\n",
    "#)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(\n",
    "#    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "#    batch_size=BATCH_SIZE, shuffle=True, **kwargs)               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kmer_path = \"data\\kmers-gzip\\\\output.txt.gz\"\n",
    "\n",
    "kmer_all = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"], index_col = 0)\n",
    "\n",
    "#kmers = kmer_all.set_index(\"kmer\", drop=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count\n",
      "count  3.388926e+07\n",
      "mean   5.418934e+01\n",
      "std    9.312541e+01\n",
      "min    2.000000e+00\n",
      "25%    3.000000e+00\n",
      "50%    1.000000e+01\n",
      "75%    5.200000e+01\n",
      "max    8.817000e+03\n",
      "             count\n",
      "count  8763.000000\n",
      "mean    923.065617\n",
      "std     320.572167\n",
      "min     700.000000\n",
      "25%     780.000000\n",
      "50%     851.000000\n",
      "75%     968.000000\n",
      "max    8817.000000\n"
     ]
    }
   ],
   "source": [
    "kmer_culled = kmer_all.loc[kmer_all['count'] >= 700].copy()\n",
    "print(kmer_all.describe())\n",
    "\n",
    "print(kmer_culled.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 count\n",
      "kmer                                  \n",
      "AAAAAACTCAGGCCGCAGTCGGTAACCTCGC      4\n",
      "AAAAAAGAATAAAACTCAGTCTTGGGGGACT     10\n",
      "AAAAACACCATCATACACTAAATCAGTAAGT     13\n",
      "AAAAACATGATCACCGGTGCTGCTCAGATGG      3\n",
      "AAAAACCTCCGGCTATGCCGGAGGATATTTA     26\n",
      "...                                ...\n",
      "TTTTGATTTCTTGTTTGACACTTGCAGTTGC     26\n",
      "TTTTGCCGTTACGCACCACCCCGTCAGTAGC     12\n",
      "TTTTGTAGGCCGGATAAGGCGTTCACGCCGC      3\n",
      "TTTTGTCTTATTCAAAGGCCTTACATTTCAA      9\n",
      "TTTTTGAAATGTAAGGCCTTTGAATAAGACA      9\n",
      "\n",
      "[2856 rows x 1 columns]\n",
      "                                count\n",
      "kmer                                 \n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA     0\n",
      "AAAAAAAGCGCCAGGAGCTGGAGAAAAAACG     0\n",
      "AAAAAACTCAGGCCGCAGTCGGTAACCTCGC     0\n",
      "AAAAAAGAATAAAACTCAGTCTTGGGGGACT     0\n",
      "AAAAAAGCGCCAGGAGCTGGAGAAAAAACGA     0\n",
      "...                               ...\n",
      "AAAAGCGCCAGGAGCTGGAGAAAAAACGAAA     0\n",
      "AAAAGCTCGTGGTATCACCATCAACACTTCT     0\n",
      "AAAAGGCTACCGCAAATCTGGTAATGAGGCC     0\n",
      "AAAAGGCTGCCTCATCGCTAACTTTGCAACA     0\n",
      "AAAAGGGGATGATAAGTTTATCACCACCGAC     0\n",
      "\n",
      "[70 rows x 1 columns]\n",
      "count    0\n",
      "Name: AAAAACTCTGCTTACCAGGCGCATTTCGCCC, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kmer_path = \"data\\kmers-gzip\\\\s314.txt.gz\"\n",
    "kmer_path2 = \"data\\kmers-gzip\\\\upec-261.txt.gz\"\n",
    "\n",
    "\n",
    "kmer_contig1 = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"], index_col = 0)\n",
    "kmer_contig2 = pd.read_csv(kmer_path2 ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"], index_col = 0)\n",
    "\n",
    "\n",
    "kmer_contig1.sort_values(by=\"kmer\", inplace=True)\n",
    "\n",
    "\n",
    "contig1_parsed = kmer_contig1[kmer_contig1.index.isin(kmer_culled.index)]\n",
    "contig2_parsed = kmer_contig2[kmer_contig2.index.isin(kmer_culled.index)]\n",
    "\n",
    "\n",
    "kmer_temp = kmer_culled.copy()\n",
    "kmer_temp['count'] = '0'\n",
    "kmer_temp.sort_values(by='kmer', inplace=True)\n",
    "print(contig1_parsed)\n",
    "print(kmer_temp.head(70))\n",
    "print(kmer_temp.loc[\"AAAAACTCTGCTTACCAGGCGCATTTCGCCC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                count_x\n",
      "kmer                                   \n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA       0\n",
      "AAAAAAAGCGCCAGGAGCTGGAGAAAAAACG       0\n",
      "AAAAAACTCAGGCCGCAGTCGGTAACCTCGC       4\n",
      "AAAAAAGAATAAAACTCAGTCTTGGGGGACT      10\n",
      "AAAAAAGCGCCAGGAGCTGGAGAAAAAACGA       0\n",
      "AAAAACACAGCACTGTGCAAACACGAAAGTG       0\n",
      "AAAAACACCATCATACACTAAATCAGTAAGT      13\n",
      "AAAAACATGATCACCGGTGCTGCTCAGATGG       3\n",
      "AAAAACCTCCGGCTATGCCGGAGGATATTTA      26\n",
      "AAAAACGGCTGCGCTGGTACTGGCGTAACCC       0\n",
      "                                count_x\n",
      "kmer                                   \n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA       0\n",
      "AAAAAAAGCGCCAGGAGCTGGAGAAAAAACG       4\n",
      "AAAAAACTCAGGCCGCAGTCGGTAACCTCGC       0\n",
      "AAAAAAGAATAAAACTCAGTCTTGGGGGACT       4\n",
      "AAAAAAGCGCCAGGAGCTGGAGAAAAAACGA       4\n",
      "AAAAACACAGCACTGTGCAAACACGAAAGTG       7\n",
      "AAAAACACCATCATACACTAAATCAGTAAGT       4\n",
      "AAAAACATGATCACCGGTGCTGCTCAGATGG       2\n",
      "AAAAACCTCCGGCTATGCCGGAGGATATTTA       0\n",
      "AAAAACGGCTGCGCTGGTACTGGCGTAACCC       6\n"
     ]
    }
   ],
   "source": [
    "contig1_merged = pd.merge(contig1_parsed, kmer_temp, how='right',on='kmer')\n",
    "contig2_merged = contig2_parsed.merge(kmer_temp, how='right',on='kmer')\n",
    "\n",
    "contig_noNull = contig1_merged[[ 'count_x']].fillna(value='0').copy()\n",
    "contig_noNull2 = contig2_merged[[ 'count_x']].fillna(value='0').copy()\n",
    "pd.isnull(contig_noNull)\n",
    "\n",
    "\n",
    "contig_noNull.sort_values(by=['kmer'], inplace=True)\n",
    "contig_noNull2.sort_values(by=['kmer'], inplace=True)\n",
    "\n",
    "#print(kmer_temp.describe())\n",
    "\n",
    "print(contig_noNull.head(10))\n",
    "print(contig_noNull2.head(10))\n",
    "#contig_clean = contig1_merged[['kmer', 'count_x']].copy()\n",
    "#contig_clean.head()\n",
    "#pd.isnull(contig_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CULL_SIZE = 700\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dirname):\n",
    "        files = os.listdir(dirname)\n",
    "        max_kmer = 0\n",
    "        X, y = [],[]\n",
    "        kmer_all = pd.read_csv('data\\kmers-gzip\\\\output.txt.gz' ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "        kmer_culled = kmer_all.loc[kmer_all['count'] >= CULL_SIZE]\n",
    "        kmer_temp = kmer_culled.copy()\n",
    "        kmer_temp['count'] = '0'\n",
    "        kmer_temp.sort_values(by=['kmer'], inplace=True)\n",
    "        self.template = kmer_temp\n",
    "        for line in files:\n",
    "            if(line != 'output.txt.gz'):\n",
    "\n",
    "                kmer_path = dirname + '\\\\' + line\n",
    "                kmer_df = pd.read_csv(kmer_path ,delim_whitespace=True, compression=\"gzip\", names = [\"kmer\", \"count\"])\n",
    "                contig_parsed = kmer_df[kmer_df.kmer.isin(self.template.kmer)]\n",
    "                contig_merged = contig_parsed.merge(self.template, how='right',on='kmer')\n",
    "\n",
    "                contig_final = contig_merged[['kmer', 'count_x']].fillna(value=0).copy()\n",
    "                contig_final.sort_values(by=['kmer'], inplace=True)\n",
    "                contig_vec = torch.tensor(contig_final['count_x'].values, dtype=torch.float)\n",
    "                X.append(contig_vec)\n",
    "                contig_max = torch.max(contig_vec)\n",
    "                if(contig_max > max_kmer): max_kmer = contig_max\n",
    "                \n",
    "        self.X = X\n",
    "        self.max_kmer = max_kmer\n",
    "    \n",
    "    def preprocess(self, contig):\n",
    "        \n",
    "        norm_vec = torch.div(contig, self.max_kmer)\n",
    "\n",
    "        #print(contig_final.head(10))\n",
    "        \n",
    "        #print(np.isnan(contig_vec))\n",
    "        \n",
    "        return norm_vec\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.preprocess(self.X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize dataset\n",
    "dataset = KmerDataset('data\\kmers-gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0357,  ..., 0.0000, 0.0397, 0.0000])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "#Separate into train/val\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#initialize data loaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, \n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                                sampler=valid_sampler)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0data: tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0079, 0.0198, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 1data: tensor([[0.0000, 0.0238, 0.0000,  ..., 0.0238, 0.0000, 0.0238]],\n",
      "       dtype=torch.float64)\n",
      "index 2data: tensor([[0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0278, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 3data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0278, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 4data: tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 5data: tensor([[0.0000, 0.0000, 0.0278,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 6data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0317, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 7data: tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0119, 0.0000, 0.0119]],\n",
      "       dtype=torch.float64)\n",
      "index 8data: tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0119, 0.0000, 0.0119]],\n",
      "       dtype=torch.float64)\n",
      "index 9data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 10data: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "index 11data: tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0238, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 12data: tensor([[0.0000, 0.0198, 0.0000,  ..., 0.0198, 0.0000, 0.0198]],\n",
      "       dtype=torch.float64)\n",
      "index 13data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0556, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 14data: tensor([[0.0000, 0.0119, 0.0397,  ..., 0.0119, 0.0000, 0.0119]],\n",
      "       dtype=torch.float64)\n",
      "index 15data: tensor([[0.0000, 0.0238, 0.0159,  ..., 0.0238, 0.0000, 0.0238]],\n",
      "       dtype=torch.float64)\n",
      "index 16data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 17data: tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0079, 0.0238, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 18data: tensor([[0.3889, 0.0000, 0.0119,  ..., 0.0000, 0.0119, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 19data: tensor([[0.0000, 0.0317, 0.0000,  ..., 0.0317, 0.0198, 0.0317]],\n",
      "       dtype=torch.float64)\n",
      "index 20data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 21data: tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0079, 0.0278, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 22data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0437, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 23data: tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0437, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 24data: tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0198, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 25data: tensor([[0.0000, 0.0159, 0.0079,  ..., 0.0159, 0.0000, 0.0159]],\n",
      "       dtype=torch.float64)\n",
      "index 26data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0079, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 27data: tensor([[0.4127, 0.0000, 0.0198,  ..., 0.0000, 0.0278, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 28data: tensor([[0.0000, 0.0278, 0.0595,  ..., 0.0278, 0.0000, 0.0278]],\n",
      "       dtype=torch.float64)\n",
      "index 29data: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "index 30data: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "index 31data: tensor([[0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 32data: tensor([[0.0000, 0.0317, 0.0000,  ..., 0.0317, 0.0159, 0.0317]],\n",
      "       dtype=torch.float64)\n",
      "index 33data: tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0119, 0.0000, 0.0119]],\n",
      "       dtype=torch.float64)\n",
      "index 34data: tensor([[0.0000, 0.0119, 0.0159,  ..., 0.0119, 0.0198, 0.0119]],\n",
      "       dtype=torch.float64)\n",
      "index 35data: tensor([[0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0278, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 36data: tensor([[0.3889, 0.0000, 0.0238,  ..., 0.0000, 0.0357, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 37data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 38data: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "index 39data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0238, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 40data: tensor([[0.2817, 0.0000, 0.0000,  ..., 0.0000, 0.0516, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 41data: tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0159, 0.0000, 0.0159]],\n",
      "       dtype=torch.float64)\n",
      "index 42data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0198, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 43data: tensor([[0.2024, 0.0000, 0.0079,  ..., 0.0000, 0.0198, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 44data: tensor([[0.0000, 0.0159, 0.0119,  ..., 0.0159, 0.0000, 0.0159]],\n",
      "       dtype=torch.float64)\n",
      "index 45data: tensor([[0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 46data: tensor([[0.0000, 0.0397, 0.0437,  ..., 0.0397, 0.0238, 0.0397]],\n",
      "       dtype=torch.float64)\n",
      "index 47data: tensor([[0.3889, 0.0079, 0.0079,  ..., 0.0079, 0.0119, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 48data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0476, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 49data: tensor([[0.0000, 0.0198, 0.0159,  ..., 0.0198, 0.0000, 0.0198]],\n",
      "       dtype=torch.float64)\n",
      "index 50data: tensor([[0.0000, 0.0000, 0.0317,  ..., 0.0000, 0.0238, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 51data: tensor([[0.0000, 0.0198, 0.0000,  ..., 0.0198, 0.0000, 0.0198]],\n",
      "       dtype=torch.float64)\n",
      "index 52data: tensor([[0.0000, 0.0317, 0.0000,  ..., 0.0317, 0.0079, 0.0317]],\n",
      "       dtype=torch.float64)\n",
      "index 53data: tensor([[0.0000, 0.0000, 0.0357,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 54data: tensor([[0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 55data: tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0159, 0.0000, 0.0159]],\n",
      "       dtype=torch.float64)\n",
      "index 56data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0317, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 57data: tensor([[0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 58data: tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0159, 0.0159, 0.0159]],\n",
      "       dtype=torch.float64)\n",
      "index 59data: tensor([[0.3889, 0.0000, 0.0079,  ..., 0.0000, 0.0476, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 60data: tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 61data: tensor([[0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0476, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 62data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0317, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 63data: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 64data: tensor([[0.0000, 0.0079, 0.0079,  ..., 0.0079, 0.0238, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 65data: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "index 66data: tensor([[0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0317, 0.0000]],\n",
      "       dtype=torch.float64)\n",
      "index 67data: tensor([[0.0000, 0.0079, 0.0159,  ..., 0.0079, 0.0159, 0.0079]],\n",
      "       dtype=torch.float64)\n",
      "index 68data: tensor([[0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0357, 0.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(test_loader):\n",
    "    \n",
    "    print('index ' + str(i[0]) + 'data: ' + str(i[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Variable is deprecated update at some point\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(8763, 400)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(400, ZDIMS)\n",
    "        self.fc22 = nn.Linear(400, ZDIMS)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(ZDIMS, 400)\n",
    "        \n",
    "        self.fc4 = nn.Linear(400, 8763)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    #if in training creates random vector based on mean and stddev\n",
    "    #if in otherwise returns constant mean \n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        \n",
    "        if self.training:\n",
    "            \n",
    "            std = logvar.mul(.5).exp_()\n",
    "            \n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            \n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "        \n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        #feed forward for the network\n",
    "        \n",
    "        #encodes data into mean(mu) and varience(logvar)\n",
    "        mu, logvar = self.encode(x.view(-1, 8763))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 8763))\n",
    "    \n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    KLD /= BATCH_SIZE * 8763\n",
    "    \n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for i in enumerate(train_loader):\n",
    "        data = i[1]\n",
    "        batch_idx = i[0]\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len (train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.data.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    for i in enumerate(test_loader):\n",
    "        data = i[1]\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        \n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data.item()\n",
    "        \n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/347 (0%)]\tLoss: 0.078007\n",
      "Train Epoch: 1 [10/347 (4%)]\tLoss: 0.033155\n",
      "Train Epoch: 1 [20/347 (7%)]\tLoss: 0.039076\n",
      "Train Epoch: 1 [30/347 (11%)]\tLoss: 0.035656\n",
      "Train Epoch: 1 [40/347 (14%)]\tLoss: 0.035119\n",
      "Train Epoch: 1 [50/347 (18%)]\tLoss: 0.035899\n",
      "Train Epoch: 1 [60/347 (22%)]\tLoss: 0.072651\n",
      "Train Epoch: 1 [70/347 (25%)]\tLoss: 0.115288\n",
      "Train Epoch: 1 [80/347 (29%)]\tLoss: 0.068152\n",
      "Train Epoch: 1 [90/347 (32%)]\tLoss: 0.021965\n",
      "Train Epoch: 1 [100/347 (36%)]\tLoss: 0.045002\n",
      "Train Epoch: 1 [110/347 (40%)]\tLoss: 0.066005\n",
      "Train Epoch: 1 [120/347 (43%)]\tLoss: 0.070047\n",
      "Train Epoch: 1 [130/347 (47%)]\tLoss: 0.029782\n",
      "Train Epoch: 1 [140/347 (50%)]\tLoss: 0.032460\n",
      "Train Epoch: 1 [150/347 (54%)]\tLoss: 0.035219\n",
      "Train Epoch: 1 [160/347 (58%)]\tLoss: 0.055389\n",
      "Train Epoch: 1 [170/347 (61%)]\tLoss: 0.039869\n",
      "Train Epoch: 1 [180/347 (65%)]\tLoss: 0.068711\n",
      "Train Epoch: 1 [190/347 (68%)]\tLoss: 0.057719\n",
      "Train Epoch: 1 [200/347 (72%)]\tLoss: 0.029238\n",
      "Train Epoch: 1 [210/347 (76%)]\tLoss: 0.051707\n",
      "Train Epoch: 1 [220/347 (79%)]\tLoss: 0.038016\n",
      "Train Epoch: 1 [230/347 (83%)]\tLoss: 0.039481\n",
      "Train Epoch: 1 [240/347 (86%)]\tLoss: 0.071772\n",
      "Train Epoch: 1 [250/347 (90%)]\tLoss: 0.054769\n",
      "Train Epoch: 1 [260/347 (94%)]\tLoss: 0.025222\n",
      "Train Epoch: 1 [270/347 (97%)]\tLoss: 0.087496\n",
      "====> Epoch: 1 Average loss: 0.0434\n",
      "====> Test set loss: 0.0105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\Anaconda3\\envs\\mGWAS\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 2 [0/347 (0%)]\tLoss: 0.039856\n",
      "Train Epoch: 2 [10/347 (4%)]\tLoss: 0.029749\n",
      "Train Epoch: 2 [20/347 (7%)]\tLoss: 0.033651\n",
      "Train Epoch: 2 [30/347 (11%)]\tLoss: 0.036625\n",
      "Train Epoch: 2 [40/347 (14%)]\tLoss: 0.067918\n",
      "Train Epoch: 2 [50/347 (18%)]\tLoss: 0.033351\n",
      "Train Epoch: 2 [60/347 (22%)]\tLoss: 0.077803\n",
      "Train Epoch: 2 [70/347 (25%)]\tLoss: 0.031333\n",
      "Train Epoch: 2 [80/347 (29%)]\tLoss: 0.056931\n",
      "Train Epoch: 2 [90/347 (32%)]\tLoss: 0.086439\n",
      "Train Epoch: 2 [100/347 (36%)]\tLoss: 0.067550\n",
      "Train Epoch: 2 [110/347 (40%)]\tLoss: 0.024527\n",
      "Train Epoch: 2 [120/347 (43%)]\tLoss: 0.067019\n",
      "Train Epoch: 2 [130/347 (47%)]\tLoss: 0.030009\n",
      "Train Epoch: 2 [140/347 (50%)]\tLoss: 0.060430\n",
      "Train Epoch: 2 [150/347 (54%)]\tLoss: 0.032607\n",
      "Train Epoch: 2 [160/347 (58%)]\tLoss: 0.066015\n",
      "Train Epoch: 2 [170/347 (61%)]\tLoss: 0.081612\n",
      "Train Epoch: 2 [180/347 (65%)]\tLoss: 0.038139\n",
      "Train Epoch: 2 [190/347 (68%)]\tLoss: 0.042660\n",
      "Train Epoch: 2 [200/347 (72%)]\tLoss: 0.060631\n",
      "Train Epoch: 2 [210/347 (76%)]\tLoss: 0.021420\n",
      "Train Epoch: 2 [220/347 (79%)]\tLoss: 0.028552\n",
      "Train Epoch: 2 [230/347 (83%)]\tLoss: 0.035913\n",
      "Train Epoch: 2 [240/347 (86%)]\tLoss: 0.047598\n",
      "Train Epoch: 2 [250/347 (90%)]\tLoss: 0.034675\n",
      "Train Epoch: 2 [260/347 (94%)]\tLoss: 0.033930\n",
      "Train Epoch: 2 [270/347 (97%)]\tLoss: 0.074944\n",
      "====> Epoch: 2 Average loss: 0.0414\n",
      "====> Test set loss: 0.0114\n",
      "Train Epoch: 3 [0/347 (0%)]\tLoss: 0.034509\n",
      "Train Epoch: 3 [10/347 (4%)]\tLoss: 0.030298\n",
      "Train Epoch: 3 [20/347 (7%)]\tLoss: 0.027898\n",
      "Train Epoch: 3 [30/347 (11%)]\tLoss: 0.077890\n",
      "Train Epoch: 3 [40/347 (14%)]\tLoss: 0.038505\n",
      "Train Epoch: 3 [50/347 (18%)]\tLoss: 0.032859\n",
      "Train Epoch: 3 [60/347 (22%)]\tLoss: 0.077646\n",
      "Train Epoch: 3 [70/347 (25%)]\tLoss: 0.030324\n",
      "Train Epoch: 3 [80/347 (29%)]\tLoss: 0.033132\n",
      "Train Epoch: 3 [90/347 (32%)]\tLoss: 0.049952\n",
      "Train Epoch: 3 [100/347 (36%)]\tLoss: 0.045033\n",
      "Train Epoch: 3 [110/347 (40%)]\tLoss: 0.025689\n",
      "Train Epoch: 3 [120/347 (43%)]\tLoss: 0.039192\n",
      "Train Epoch: 3 [130/347 (47%)]\tLoss: 0.040152\n",
      "Train Epoch: 3 [140/347 (50%)]\tLoss: 0.028800\n",
      "Train Epoch: 3 [150/347 (54%)]\tLoss: 0.075658\n",
      "Train Epoch: 3 [160/347 (58%)]\tLoss: 0.036187\n",
      "Train Epoch: 3 [170/347 (61%)]\tLoss: 0.102225\n",
      "Train Epoch: 3 [180/347 (65%)]\tLoss: 0.053758\n",
      "Train Epoch: 3 [190/347 (68%)]\tLoss: 0.034521\n",
      "Train Epoch: 3 [200/347 (72%)]\tLoss: 0.023149\n",
      "Train Epoch: 3 [210/347 (76%)]\tLoss: 0.030790\n",
      "Train Epoch: 3 [220/347 (79%)]\tLoss: 0.032162\n",
      "Train Epoch: 3 [230/347 (83%)]\tLoss: 0.035111\n",
      "Train Epoch: 3 [240/347 (86%)]\tLoss: 0.057003\n",
      "Train Epoch: 3 [250/347 (90%)]\tLoss: 0.031564\n",
      "Train Epoch: 3 [260/347 (94%)]\tLoss: 0.034225\n",
      "Train Epoch: 3 [270/347 (97%)]\tLoss: 0.031271\n",
      "====> Epoch: 3 Average loss: 0.0402\n",
      "====> Test set loss: 0.0109\n",
      "Train Epoch: 4 [0/347 (0%)]\tLoss: 0.026445\n",
      "Train Epoch: 4 [10/347 (4%)]\tLoss: 0.028103\n",
      "Train Epoch: 4 [20/347 (7%)]\tLoss: 0.054450\n",
      "Train Epoch: 4 [30/347 (11%)]\tLoss: 0.029183\n",
      "Train Epoch: 4 [40/347 (14%)]\tLoss: 0.099018\n",
      "Train Epoch: 4 [50/347 (18%)]\tLoss: 0.034734\n",
      "Train Epoch: 4 [60/347 (22%)]\tLoss: 0.063806\n",
      "Train Epoch: 4 [70/347 (25%)]\tLoss: 0.045163\n",
      "Train Epoch: 4 [80/347 (29%)]\tLoss: 0.058078\n",
      "Train Epoch: 4 [90/347 (32%)]\tLoss: 0.040496\n",
      "Train Epoch: 4 [100/347 (36%)]\tLoss: 0.031967\n",
      "Train Epoch: 4 [110/347 (40%)]\tLoss: 0.055074\n",
      "Train Epoch: 4 [120/347 (43%)]\tLoss: 0.070391\n",
      "Train Epoch: 4 [130/347 (47%)]\tLoss: 0.029298\n",
      "Train Epoch: 4 [140/347 (50%)]\tLoss: 0.033612\n",
      "Train Epoch: 4 [150/347 (54%)]\tLoss: 0.059829\n",
      "Train Epoch: 4 [160/347 (58%)]\tLoss: 0.032680\n",
      "Train Epoch: 4 [170/347 (61%)]\tLoss: 0.031059\n",
      "Train Epoch: 4 [180/347 (65%)]\tLoss: 0.091097\n",
      "Train Epoch: 4 [190/347 (68%)]\tLoss: 0.046606\n",
      "Train Epoch: 4 [200/347 (72%)]\tLoss: 0.030166\n",
      "Train Epoch: 4 [210/347 (76%)]\tLoss: 0.033651\n",
      "Train Epoch: 4 [220/347 (79%)]\tLoss: 0.035467\n",
      "Train Epoch: 4 [230/347 (83%)]\tLoss: 0.067852\n",
      "Train Epoch: 4 [240/347 (86%)]\tLoss: 0.026570\n",
      "Train Epoch: 4 [250/347 (90%)]\tLoss: 0.021735\n",
      "Train Epoch: 4 [260/347 (94%)]\tLoss: 0.088611\n",
      "Train Epoch: 4 [270/347 (97%)]\tLoss: 0.022238\n",
      "====> Epoch: 4 Average loss: 0.0395\n",
      "====> Test set loss: 0.0109\n",
      "Train Epoch: 5 [0/347 (0%)]\tLoss: 0.056896\n",
      "Train Epoch: 5 [10/347 (4%)]\tLoss: 0.034934\n",
      "Train Epoch: 5 [20/347 (7%)]\tLoss: 0.032316\n",
      "Train Epoch: 5 [30/347 (11%)]\tLoss: 0.050988\n",
      "Train Epoch: 5 [40/347 (14%)]\tLoss: 0.075580\n",
      "Train Epoch: 5 [50/347 (18%)]\tLoss: 0.034604\n",
      "Train Epoch: 5 [60/347 (22%)]\tLoss: 0.069636\n",
      "Train Epoch: 5 [70/347 (25%)]\tLoss: 0.028735\n",
      "Train Epoch: 5 [80/347 (29%)]\tLoss: 0.024473\n",
      "Train Epoch: 5 [90/347 (32%)]\tLoss: 0.083614\n",
      "Train Epoch: 5 [100/347 (36%)]\tLoss: 0.083221\n",
      "Train Epoch: 5 [110/347 (40%)]\tLoss: 0.033246\n",
      "Train Epoch: 5 [120/347 (43%)]\tLoss: 0.028625\n",
      "Train Epoch: 5 [130/347 (47%)]\tLoss: 0.032976\n",
      "Train Epoch: 5 [140/347 (50%)]\tLoss: 0.078364\n",
      "Train Epoch: 5 [150/347 (54%)]\tLoss: 0.074315\n",
      "Train Epoch: 5 [160/347 (58%)]\tLoss: 0.078147\n",
      "Train Epoch: 5 [170/347 (61%)]\tLoss: 0.034621\n",
      "Train Epoch: 5 [180/347 (65%)]\tLoss: 0.042098\n",
      "Train Epoch: 5 [190/347 (68%)]\tLoss: 0.064824\n",
      "Train Epoch: 5 [200/347 (72%)]\tLoss: 0.033779\n",
      "Train Epoch: 5 [210/347 (76%)]\tLoss: 0.026588\n",
      "Train Epoch: 5 [220/347 (79%)]\tLoss: 0.034021\n",
      "Train Epoch: 5 [230/347 (83%)]\tLoss: 0.051583\n",
      "Train Epoch: 5 [240/347 (86%)]\tLoss: 0.030814\n",
      "Train Epoch: 5 [250/347 (90%)]\tLoss: 0.028434\n",
      "Train Epoch: 5 [260/347 (94%)]\tLoss: 0.043225\n",
      "Train Epoch: 5 [270/347 (97%)]\tLoss: 0.026311\n",
      "====> Epoch: 5 Average loss: 0.0390\n",
      "====> Test set loss: 0.0105\n",
      "Train Epoch: 6 [0/347 (0%)]\tLoss: 0.056864\n",
      "Train Epoch: 6 [10/347 (4%)]\tLoss: 0.028602\n",
      "Train Epoch: 6 [20/347 (7%)]\tLoss: 0.030061\n",
      "Train Epoch: 6 [30/347 (11%)]\tLoss: 0.067375\n",
      "Train Epoch: 6 [40/347 (14%)]\tLoss: 0.035957\n",
      "Train Epoch: 6 [50/347 (18%)]\tLoss: 0.078580\n",
      "Train Epoch: 6 [60/347 (22%)]\tLoss: 0.104235\n",
      "Train Epoch: 6 [70/347 (25%)]\tLoss: 0.101672\n",
      "Train Epoch: 6 [80/347 (29%)]\tLoss: 0.028312\n",
      "Train Epoch: 6 [90/347 (32%)]\tLoss: 0.064975\n",
      "Train Epoch: 6 [100/347 (36%)]\tLoss: 0.028949\n",
      "Train Epoch: 6 [110/347 (40%)]\tLoss: 0.022421\n",
      "Train Epoch: 6 [120/347 (43%)]\tLoss: 0.028530\n",
      "Train Epoch: 6 [130/347 (47%)]\tLoss: 0.052813\n",
      "Train Epoch: 6 [140/347 (50%)]\tLoss: 0.027985\n",
      "Train Epoch: 6 [150/347 (54%)]\tLoss: 0.035725\n",
      "Train Epoch: 6 [160/347 (58%)]\tLoss: 0.069177\n",
      "Train Epoch: 6 [170/347 (61%)]\tLoss: 0.050583\n",
      "Train Epoch: 6 [180/347 (65%)]\tLoss: 0.021845\n",
      "Train Epoch: 6 [190/347 (68%)]\tLoss: 0.077129\n",
      "Train Epoch: 6 [200/347 (72%)]\tLoss: 0.055003\n",
      "Train Epoch: 6 [210/347 (76%)]\tLoss: 0.029309\n",
      "Train Epoch: 6 [220/347 (79%)]\tLoss: 0.033532\n",
      "Train Epoch: 6 [230/347 (83%)]\tLoss: 0.043917\n",
      "Train Epoch: 6 [240/347 (86%)]\tLoss: 0.103632\n",
      "Train Epoch: 6 [250/347 (90%)]\tLoss: 0.070229\n",
      "Train Epoch: 6 [260/347 (94%)]\tLoss: 0.060185\n",
      "Train Epoch: 6 [270/347 (97%)]\tLoss: 0.076640\n",
      "====> Epoch: 6 Average loss: 0.0390\n",
      "====> Test set loss: 0.0098\n",
      "Train Epoch: 7 [0/347 (0%)]\tLoss: 0.027306\n",
      "Train Epoch: 7 [10/347 (4%)]\tLoss: 0.062528\n",
      "Train Epoch: 7 [20/347 (7%)]\tLoss: 0.026408\n",
      "Train Epoch: 7 [30/347 (11%)]\tLoss: 0.055405\n",
      "Train Epoch: 7 [40/347 (14%)]\tLoss: 0.036263\n",
      "Train Epoch: 7 [50/347 (18%)]\tLoss: 0.019006\n",
      "Train Epoch: 7 [60/347 (22%)]\tLoss: 0.026792\n",
      "Train Epoch: 7 [70/347 (25%)]\tLoss: 0.046225\n",
      "Train Epoch: 7 [80/347 (29%)]\tLoss: 0.060546\n",
      "Train Epoch: 7 [90/347 (32%)]\tLoss: 0.032823\n",
      "Train Epoch: 7 [100/347 (36%)]\tLoss: 0.050274\n",
      "Train Epoch: 7 [110/347 (40%)]\tLoss: 0.026200\n",
      "Train Epoch: 7 [120/347 (43%)]\tLoss: 0.066336\n",
      "Train Epoch: 7 [130/347 (47%)]\tLoss: 0.087748\n",
      "Train Epoch: 7 [140/347 (50%)]\tLoss: 0.022065\n",
      "Train Epoch: 7 [150/347 (54%)]\tLoss: 0.101465\n",
      "Train Epoch: 7 [160/347 (58%)]\tLoss: 0.044541\n",
      "Train Epoch: 7 [170/347 (61%)]\tLoss: 0.103051\n",
      "Train Epoch: 7 [180/347 (65%)]\tLoss: 0.037837\n",
      "Train Epoch: 7 [190/347 (68%)]\tLoss: 0.101955\n",
      "Train Epoch: 7 [200/347 (72%)]\tLoss: 0.026507\n",
      "Train Epoch: 7 [210/347 (76%)]\tLoss: 0.021892\n",
      "Train Epoch: 7 [220/347 (79%)]\tLoss: 0.030657\n",
      "Train Epoch: 7 [230/347 (83%)]\tLoss: 0.038767\n",
      "Train Epoch: 7 [240/347 (86%)]\tLoss: 0.027834\n",
      "Train Epoch: 7 [250/347 (90%)]\tLoss: 0.045064\n",
      "Train Epoch: 7 [260/347 (94%)]\tLoss: 0.028634\n",
      "Train Epoch: 7 [270/347 (97%)]\tLoss: 0.022301\n",
      "====> Epoch: 7 Average loss: 0.0387\n",
      "====> Test set loss: 0.0097\n",
      "Train Epoch: 8 [0/347 (0%)]\tLoss: 0.033712\n",
      "Train Epoch: 8 [10/347 (4%)]\tLoss: 0.043763\n",
      "Train Epoch: 8 [20/347 (7%)]\tLoss: 0.057484\n",
      "Train Epoch: 8 [30/347 (11%)]\tLoss: 0.065769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [40/347 (14%)]\tLoss: 0.033140\n",
      "Train Epoch: 8 [50/347 (18%)]\tLoss: 0.077920\n",
      "Train Epoch: 8 [60/347 (22%)]\tLoss: 0.057592\n",
      "Train Epoch: 8 [70/347 (25%)]\tLoss: 0.031187\n",
      "Train Epoch: 8 [80/347 (29%)]\tLoss: 0.092030\n",
      "Train Epoch: 8 [90/347 (32%)]\tLoss: 0.043460\n",
      "Train Epoch: 8 [100/347 (36%)]\tLoss: 0.028127\n",
      "Train Epoch: 8 [110/347 (40%)]\tLoss: 0.065867\n",
      "Train Epoch: 8 [120/347 (43%)]\tLoss: 0.115663\n",
      "Train Epoch: 8 [130/347 (47%)]\tLoss: 0.032034\n",
      "Train Epoch: 8 [140/347 (50%)]\tLoss: 0.031380\n",
      "Train Epoch: 8 [150/347 (54%)]\tLoss: 0.052352\n",
      "Train Epoch: 8 [160/347 (58%)]\tLoss: 0.043095\n",
      "Train Epoch: 8 [170/347 (61%)]\tLoss: 0.023582\n",
      "Train Epoch: 8 [180/347 (65%)]\tLoss: 0.030786\n",
      "Train Epoch: 8 [190/347 (68%)]\tLoss: 0.065708\n",
      "Train Epoch: 8 [200/347 (72%)]\tLoss: 0.087520\n",
      "Train Epoch: 8 [210/347 (76%)]\tLoss: 0.024732\n",
      "Train Epoch: 8 [220/347 (79%)]\tLoss: 0.038753\n",
      "Train Epoch: 8 [230/347 (83%)]\tLoss: 0.025223\n",
      "Train Epoch: 8 [240/347 (86%)]\tLoss: 0.027710\n",
      "Train Epoch: 8 [250/347 (90%)]\tLoss: 0.022684\n",
      "Train Epoch: 8 [260/347 (94%)]\tLoss: 0.053683\n",
      "Train Epoch: 8 [270/347 (97%)]\tLoss: 0.030511\n",
      "====> Epoch: 8 Average loss: 0.0386\n",
      "====> Test set loss: 0.0099\n",
      "Train Epoch: 9 [0/347 (0%)]\tLoss: 0.052393\n",
      "Train Epoch: 9 [10/347 (4%)]\tLoss: 0.088531\n",
      "Train Epoch: 9 [20/347 (7%)]\tLoss: 0.022925\n",
      "Train Epoch: 9 [30/347 (11%)]\tLoss: 0.038118\n",
      "Train Epoch: 9 [40/347 (14%)]\tLoss: 0.083624\n",
      "Train Epoch: 9 [50/347 (18%)]\tLoss: 0.032538\n",
      "Train Epoch: 9 [60/347 (22%)]\tLoss: 0.052936\n",
      "Train Epoch: 9 [70/347 (25%)]\tLoss: 0.100610\n",
      "Train Epoch: 9 [80/347 (29%)]\tLoss: 0.034675\n",
      "Train Epoch: 9 [90/347 (32%)]\tLoss: 0.034086\n",
      "Train Epoch: 9 [100/347 (36%)]\tLoss: 0.067839\n",
      "Train Epoch: 9 [110/347 (40%)]\tLoss: 0.040044\n",
      "Train Epoch: 9 [120/347 (43%)]\tLoss: 0.050322\n",
      "Train Epoch: 9 [130/347 (47%)]\tLoss: 0.030871\n",
      "Train Epoch: 9 [140/347 (50%)]\tLoss: 0.022377\n",
      "Train Epoch: 9 [150/347 (54%)]\tLoss: 0.070090\n",
      "Train Epoch: 9 [160/347 (58%)]\tLoss: 0.054074\n",
      "Train Epoch: 9 [170/347 (61%)]\tLoss: 0.097131\n",
      "Train Epoch: 9 [180/347 (65%)]\tLoss: 0.077050\n",
      "Train Epoch: 9 [190/347 (68%)]\tLoss: 0.025551\n",
      "Train Epoch: 9 [200/347 (72%)]\tLoss: 0.034650\n",
      "Train Epoch: 9 [210/347 (76%)]\tLoss: 0.084533\n",
      "Train Epoch: 9 [220/347 (79%)]\tLoss: 0.064658\n",
      "Train Epoch: 9 [230/347 (83%)]\tLoss: 0.077899\n",
      "Train Epoch: 9 [240/347 (86%)]\tLoss: 0.068457\n",
      "Train Epoch: 9 [250/347 (90%)]\tLoss: 0.108252\n",
      "Train Epoch: 9 [260/347 (94%)]\tLoss: 0.052600\n",
      "Train Epoch: 9 [270/347 (97%)]\tLoss: 0.027939\n",
      "====> Epoch: 9 Average loss: 0.0383\n",
      "====> Test set loss: 0.0094\n",
      "Train Epoch: 10 [0/347 (0%)]\tLoss: 0.033273\n",
      "Train Epoch: 10 [10/347 (4%)]\tLoss: 0.022359\n",
      "Train Epoch: 10 [20/347 (7%)]\tLoss: 0.028929\n",
      "Train Epoch: 10 [30/347 (11%)]\tLoss: 0.030095\n",
      "Train Epoch: 10 [40/347 (14%)]\tLoss: 0.026555\n",
      "Train Epoch: 10 [50/347 (18%)]\tLoss: 0.044614\n",
      "Train Epoch: 10 [60/347 (22%)]\tLoss: 0.074019\n",
      "Train Epoch: 10 [70/347 (25%)]\tLoss: 0.020936\n",
      "Train Epoch: 10 [80/347 (29%)]\tLoss: 0.030883\n",
      "Train Epoch: 10 [90/347 (32%)]\tLoss: 0.053729\n",
      "Train Epoch: 10 [100/347 (36%)]\tLoss: 0.088836\n",
      "Train Epoch: 10 [110/347 (40%)]\tLoss: 0.094593\n",
      "Train Epoch: 10 [120/347 (43%)]\tLoss: 0.039719\n",
      "Train Epoch: 10 [130/347 (47%)]\tLoss: 0.049568\n",
      "Train Epoch: 10 [140/347 (50%)]\tLoss: 0.032025\n",
      "Train Epoch: 10 [150/347 (54%)]\tLoss: 0.050025\n",
      "Train Epoch: 10 [160/347 (58%)]\tLoss: 0.056465\n",
      "Train Epoch: 10 [170/347 (61%)]\tLoss: 0.022169\n",
      "Train Epoch: 10 [180/347 (65%)]\tLoss: 0.081774\n",
      "Train Epoch: 10 [190/347 (68%)]\tLoss: 0.022902\n",
      "Train Epoch: 10 [200/347 (72%)]\tLoss: 0.023322\n",
      "Train Epoch: 10 [210/347 (76%)]\tLoss: 0.027454\n",
      "Train Epoch: 10 [220/347 (79%)]\tLoss: 0.060200\n",
      "Train Epoch: 10 [230/347 (83%)]\tLoss: 0.095697\n",
      "Train Epoch: 10 [240/347 (86%)]\tLoss: 0.027706\n",
      "Train Epoch: 10 [250/347 (90%)]\tLoss: 0.037800\n",
      "Train Epoch: 10 [260/347 (94%)]\tLoss: 0.052208\n",
      "Train Epoch: 10 [270/347 (97%)]\tLoss: 0.079002\n",
      "====> Epoch: 10 Average loss: 0.0382\n",
      "====> Test set loss: 0.0091\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "    \n",
    "    sample = Variable(torch.randn(64, ZDIMS))\n",
    "    \n",
    "    if CUDA:\n",
    "        sample = sample.cuda()\n",
    "    \n",
    "    sample = model.decode(sample).cpu()\n",
    "    \n",
    "    \n",
    "    #save_image(sample.data.view(64, 1, 28, 28),\n",
    "              #'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
